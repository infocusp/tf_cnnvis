{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import time\n",
    "import copy\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from subprocess import call\n",
    "from scipy.misc import imread, imresize\n",
    "from tf_cnnvis import get_visualization, image_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"python -m wget -o ./alexnet_weights.h5 http://files.heuritech.com/weights/alexnet_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All images in './sample_images\" directory\n",
    "def load_images(output_shape = (224, 224), path = \"./sample_images/\"):\n",
    "    filenames = os.listdir(path)\n",
    "    im = np.zeros((len(filenames), output_shape[0], output_shape[1], 3))\n",
    "\n",
    "    i = 0\n",
    "    for file in filenames:\n",
    "        im[i] = image_normalization(imresize(imread(os.path.join(path, file)), output_shape)) / 255.0\n",
    "        i += 1\n",
    "            \n",
    "    return im, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "Unable to open file (Truncated file: eof = 30261256, sblock->base_addr = 0, stored_eoa = 243904576)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-34a7af94a50e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./img_mean.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load mean image of imagenet dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./alexnet_weights.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconv_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conv_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conv_1_W\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conv_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conv_1_b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/tmp/pip-4rPeHA-build/h5py/_objects.c:2684)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/tmp/pip-4rPeHA-build/h5py/_objects.c:2642)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/tmp/pip-4rPeHA-build/h5py/h5f.c:1930)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Unable to open file (Truncated file: eof = 30261256, sblock->base_addr = 0, stored_eoa = 243904576)"
     ]
    }
   ],
   "source": [
    "# loading parameters and mean image for pretrained alexnet model\n",
    "mean = np.load(\"./img_mean.npy\").transpose((1, 2, 0)) # load mean image of imagenet dataset\n",
    "\n",
    "f = h5py.File('./alexnet_weights.h5','r')\n",
    "\n",
    "conv_1 = [f[\"conv_1\"][\"conv_1_W\"], f[\"conv_1\"][\"conv_1_b\"]]\n",
    "conv_2_1 = [f[\"conv_2_1\"][\"conv_2_1_W\"], f[\"conv_2_1\"][\"conv_2_1_b\"]]\n",
    "conv_2_2 = [f[\"conv_2_2\"][\"conv_2_2_W\"], f[\"conv_2_2\"][\"conv_2_2_b\"]]\n",
    "conv_3 = [f[\"conv_3\"][\"conv_3_W\"], f[\"conv_3\"][\"conv_3_b\"]]\n",
    "conv_4_1 = [f[\"conv_4_1\"][\"conv_4_1_W\"], f[\"conv_4_1\"][\"conv_4_1_b\"]]\n",
    "conv_4_2 = [f[\"conv_4_2\"][\"conv_4_2_W\"], f[\"conv_4_2\"][\"conv_4_2_b\"]]\n",
    "conv_5_1 = [f[\"conv_5_1\"][\"conv_5_1_W\"], f[\"conv_5_1\"][\"conv_5_1_b\"]]\n",
    "conv_5_2 = [f[\"conv_5_2\"][\"conv_5_2_W\"], f[\"conv_5_2\"][\"conv_5_2_b\"]]\n",
    "fc_6 = [f[\"dense_1\"][\"dense_1_W\"], f[\"dense_1\"][\"dense_1_b\"]]\n",
    "fc_7 = [f[\"dense_2\"][\"dense_2_W\"], f[\"dense_2\"][\"dense_2_b\"]]\n",
    "fc_8 = [f[\"dense_3\"][\"dense_3_W\"], f[\"dense_3\"][\"dense_3_b\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow model implementation (Alexnet convolution)\n",
    "X = tf.placeholder(tf.float32, shape = [None, 224, 224, 3]) # placeholder for input images\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 1000]) # placeholder for true labels for input images\n",
    "\n",
    "radius = 5; alpha = 1e-4; beta = 0.75; bias = 2.0 # hyper parametes for lrn\n",
    "\n",
    "\n",
    "# Layer - 1 conv1\n",
    "W_conv_1 = tf.Variable(np.transpose(conv_1[0], (2, 3, 1, 0)))\n",
    "b_conv_1 = tf.Variable(np.reshape(conv_1[1], (96, )))\n",
    "\n",
    "y_conv_1 = tf.nn.conv2d(X, filter=W_conv_1, strides=[1, 4, 4, 1], padding=\"SAME\") + b_conv_1\n",
    "h_conv_1 = tf.nn.relu(y_conv_1, name = \"conv1\")\n",
    "h_conv_1 = tf.nn.local_response_normalization(h_conv_1, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
    "h_pool_1 = tf.nn.max_pool(h_conv_1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "\n",
    "h_pool_1_1, h_pool_1_2 = tf.split(axis = 3, value = h_pool_1, num_or_size_splits = 2)\n",
    "\n",
    "\n",
    "\n",
    "# Layer - 2 conv2\n",
    "W_conv_2_1 = tf.Variable(np.transpose(conv_2_1[0], (2, 3, 1, 0)))\n",
    "b_conv_2_1 = tf.Variable(np.reshape(conv_2_1[1], (128, )))\n",
    "\n",
    "y_conv_2_1 = tf.nn.conv2d(h_pool_1_1, filter=W_conv_2_1, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_2_1\n",
    "h_conv_2_1 = tf.nn.relu(y_conv_2_1, name = \"conv2_1\")\n",
    "h_conv_2_1 = tf.nn.local_response_normalization(h_conv_2_1, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
    "h_pool_2_1 = tf.nn.max_pool(h_conv_2_1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "\n",
    "\n",
    "W_conv_2_2 = tf.Variable(np.transpose(conv_2_2[0], (2, 3, 1, 0)))\n",
    "b_conv_2_2 = tf.Variable(np.reshape(conv_2_2[1], (128, )))\n",
    "\n",
    "y_conv_2_2 = tf.nn.conv2d(h_pool_1_2, filter=W_conv_2_2, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_2_2\n",
    "h_conv_2_2 = tf.nn.relu(y_conv_2_2, name = \"conv2_2\")\n",
    "h_conv_2_2 = tf.nn.local_response_normalization(h_conv_2_2, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
    "h_pool_2_2 = tf.nn.max_pool(h_conv_2_2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "\n",
    "h_pool_2 = tf.concat(axis = 3, values = [h_pool_2_1, h_pool_2_2])\n",
    "\n",
    "\n",
    "\n",
    "# Layer - 3 conv3\n",
    "W_conv_3 = tf.Variable(np.transpose(conv_3[0], (2, 3, 1, 0)))\n",
    "b_conv_3 = tf.Variable(np.reshape(conv_3[1], (384, )))\n",
    "\n",
    "y_conv_3 = tf.nn.conv2d(h_pool_2, filter=W_conv_3, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_3\n",
    "h_conv_3 = tf.nn.relu(y_conv_3, name = \"conv3\")\n",
    "\n",
    "h_conv_3_1, h_conv_3_2 = tf.split(axis = 3, value = h_conv_3, num_or_size_splits = 2)\n",
    "\n",
    "\n",
    "# h_conv_4_1 = h_conv_3_1\n",
    "# h_conv_4_2 = h_conv_3_2\n",
    "# Layer - 4 conv4\n",
    "W_conv_4_1 = tf.Variable(np.transpose(conv_4_1[0], (2, 3, 1, 0)))\n",
    "b_conv_4_1 = tf.Variable(np.reshape(conv_4_1[1], (192, )))\n",
    "\n",
    "y_conv_4_1 = tf.nn.conv2d(h_conv_3_1, filter=W_conv_4_1, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_4_1\n",
    "h_conv_4_1 = tf.nn.relu(y_conv_4_1, name = \"conv4_1\")\n",
    "\n",
    "\n",
    "W_conv_4_2 = tf.Variable(np.transpose(conv_4_2[0], (2, 3, 1, 0)))\n",
    "b_conv_4_2 = tf.Variable(np.reshape(conv_4_2[1], (192, )))\n",
    "\n",
    "y_conv_4_2 = tf.nn.conv2d(h_conv_3_2, filter=W_conv_4_2, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_4_2\n",
    "h_conv_4_2 = tf.nn.relu(y_conv_4_2, name = \"conv4_2\")\n",
    "\n",
    "h_conv_4 = tf.concat(axis = 3, values = [h_conv_4_1, h_conv_4_2])\n",
    "\n",
    "\n",
    "\n",
    "# Layer - 5 conv5\n",
    "W_conv_5_1 = tf.Variable(np.transpose(conv_5_1[0], (2, 3, 1, 0)))\n",
    "b_conv_5_1 = tf.Variable(np.reshape(conv_5_1[1], (128, )))\n",
    "\n",
    "y_conv_5_1 = tf.nn.conv2d(h_conv_4_1, filter=W_conv_5_1, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_2_1\n",
    "h_conv_5_1 = tf.nn.relu(y_conv_5_1, name = \"conv5_1\")\n",
    "h_conv_5_1 = tf.nn.local_response_normalization(h_conv_5_1, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
    "h_pool_5_1 = tf.nn.max_pool(h_conv_5_1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "\n",
    "\n",
    "W_conv_5_2 = tf.Variable(np.transpose(conv_5_2[0], (2, 3, 1, 0)))\n",
    "b_conv_5_2 = tf.Variable(np.reshape(conv_5_2[1], (128, )))\n",
    "\n",
    "y_conv_5_2 = tf.nn.conv2d(h_conv_4_2, filter=W_conv_5_2, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_2_2\n",
    "h_conv_5_2 = tf.nn.relu(y_conv_5_2, name = \"conv5_2\")\n",
    "h_conv_5_2 = tf.nn.local_response_normalization(h_conv_5_2, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
    "h_pool_5_2 = tf.nn.max_pool(h_conv_5_2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "\n",
    "h_pool_5 = tf.concat(axis = 3, values = [h_pool_5_1, h_pool_5_2])\n",
    "\n",
    "dimensions = h_pool_5.get_shape().as_list()\n",
    "dim = dimensions[1] * dimensions[2] * dimensions[3]\n",
    "\n",
    "# # Part of Alexnet model which is not required for deconvolution\n",
    "h_flatten = tf.reshape(h_pool_5, shape=[-1, dim])\n",
    "\n",
    "# Layer - 6 fc6\n",
    "W_full_6 = tf.Variable(np.array(fc_6[0]))\n",
    "b_full_6 = tf.Variable(np.array(fc_6[1]))\n",
    "\n",
    "y_full_6 = tf.add(tf.matmul(h_flatten, W_full_6), b_full_6)\n",
    "h_full_6 = tf.nn.relu(y_full_6, name = \"fc6\")\n",
    "h_dropout_6 = tf.nn.dropout(h_full_6, 0.5)\n",
    "\n",
    "\n",
    "# Layer - 7 fc7\n",
    "W_full_7 = tf.Variable(np.array(fc_7[0]))\n",
    "b_full_7 = tf.Variable(np.array(fc_7[1]))\n",
    "\n",
    "y_full_7 = tf.add(tf.matmul(h_dropout_6, W_full_7), b_full_7)\n",
    "h_full_7 = tf.nn.relu(y_full_7, name = \"fc7\")\n",
    "h_dropout_7 = tf.nn.dropout(h_full_7, 0.5)\n",
    "\n",
    "\n",
    "# Layer - 8 fc8\n",
    "W_full_8 = tf.Variable(np.array(fc_8[0]))\n",
    "b_full_8 = tf.Variable(np.array(fc_8[1]))\n",
    "\n",
    "y_full_8 = tf.add(tf.matmul(h_dropout_7, W_full_8), b_full_8, name = \"fc8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# im, filenames = load_images(path = \"./sample_images\")\n",
    "im = np.expand_dims(image_normalization(imresize(imread(os.path.join(\"./sample_images\", \"images.jpg\")), (224, 224))) / 255.0, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = [\"conv1\", \"conv2_1\", \"conv2_2\", \"p\"]\n",
    "total_time = 0\n",
    "\n",
    "start = time.time()\n",
    "get_visualization(graph_or_path = tf.get_default_graph(), value_feed_dict = {X : im}, layers=layers)\n",
    "start = time.time() - start\n",
    "print(\"Total Time = %f\" % (start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
